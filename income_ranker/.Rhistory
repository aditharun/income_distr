pdf("/Users/aditharun/Desktop/visualize_deconvolution2.pdf",8,8)
plot1
plot2
dev.off()
library(tidyverse)
comparisons <- readRDS("/Users/aditharun/Desktop/c.rds")
comparisons$delfi.rep.maf <- ifelse(is.na(comparisons$delfi.rep.maf), comparisons$adhoc.maf, comparisons$delfi.rep.maf)
A <- data.frame(estimate=rep(comparisons$estimate, 2), reference=c(comparisons$delfi.rep.maf,comparisons$ichor.tumor.fraction), type=rep(c("ref: maf","ref: ichor"), each=dim(comparisons)[1]))
plot1 <- ggplot(A, aes(estimate, reference)) + geom_point(size=1.7) + facet_wrap(~type) + geom_abline(slope=1,intercept=0, linetype=2) + xlab("estimated tumor fraction") + ylab("reference tumor fraction")
B <-  data.frame(tumor_fraction=c(comparisons$delfi.rep.maf,comparisons$ichor.tumor.fraction, comparisons$estimate), type=rep(c("maf","ichor","estimate"), each=dim(comparisons)[1]), pt=rep(as.character(c(1:20)),3),pt_name=rep(comparisons$id,3))
plot2 <- ggplot(B, aes(fill=type,y=tumor_fraction,x=pt)) + geom_bar(position="dodge",stat="identity") + xlab("patient") + ylab("tumor fraction")
pdf("/Users/aditharun/Desktop/visualize_deconvolution2.pdf",8,8)
plot1
plot2
dev.off()
library(shiny)
runApp("/Users/aditharun/Desktop/income/app.R")
library(tidyverse)
soi.data <- read_csv("/Users/aditharun/Downloads/15incyallagi.csv")
#the upper bound on the last interval will not matter so we can set it to the placeholder value of 0
agi <- c(0.001,1, 10000,25000,50000,75000,100000,200000,0)
#N2 according to SIS IRS lady is the number of people who filed so gives income per person
soi.data$avg.agi <- (soi.data$A00100 / soi.data$N1) * 1000
mean.pareto.bounded <- function(lower, upper, val){
v <- val
return(((lower^v) / (1 - ((lower/upper)^v))) * (v / (v - 1)) * ((1 / (lower^(v-1))) - (1 / (upper^(v-1)))))
}
#linear narrowing down should work
estimate.alpha <- function(lower, upper, average){
bounds <- c(0.00000001,50)
limits <- c(mean.pareto.bounded(lower=lower,upper=upper, val=bounds[1]), mean.pareto.bounded(lower=lower,upper=upper,val=bounds[2]))
if (sum(average < limits)!=1){
return("can't be found using bounded estimate")
}
midpt <- mean(bounds)
guess <- mean.pareto.bounded(lower=lower, upper=upper, val=midpt)
loss <- abs(guess - average)
while (loss > 100){
if (guess >= average){
bounds <- c(midpt, max(bounds))
midpt <- mean(c(midpt, max(bounds)))
} else{
bounds <- c(midpt, min(bounds))
midpt <- mean(c(midpt, min(bounds)))
}
guess <- mean.pareto.bounded(lower=lower,upper=upper, val=midpt)
loss <- abs(guess - average)
}
return(midpt)
}
income.distr <- function(data){
samples <- list()
for (x in 1:8){
sampling <- agi[x:(x+1)]
idx <- data$N1[x]
if (is.na(data$avg.agi[x])){
y <- seq(sampling[1],sampling[2],length=7000)
out <- sample(y, replace=TRUE, idx)
} else{
if (x %in% c(2,3,4,5,6,7)){
lower <- sampling[1]
upper <- sampling[2]
alpha <- estimate.alpha(lower=lower,upper=upper, average=data$avg.agi[x])
if (typeof(alpha)=="character"){
alpha <- -data$avg.agi[x] / ((sampling[1])-data$avg.agi[x])
out <- sampling[1]/((1 - runif(idx))^(1/alpha))
} else{
out <- ((lower^alpha) / (1 - (runif(idx)*(1 - ((lower/upper)^alpha)))))^(1/alpha)
}
} else if (x==8) {
#regular pareto
alpha <- -data$avg.agi[x] / ((sampling[1])-data$avg.agi[x])
out <- sampling[1]/((1 - runif(idx))^(1/alpha))
} else{
out <- rep(0, idx)
}
}
out <- sample(out, replace=FALSE,round(idx*0.5))
samples[[x]] <- out
}
samples <- unlist(samples)
lambda.exp <- 1 / mean(samples)
return(lambda.exp)
}
tax.data <- soi.data
y <- tax.data %>% group_by(COUNTYNAME, STATE) %>% select(c(STATE, COUNTYNAME, N1, avg.agi)) %>% nest() %>% mutate(param=map(data,~income.distr(.))) %>% unnest(param)
y$data <- NULL
write.csv(y, "/Users/aditharun/Desktop/other/income/modeledData_2015.csv")
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data
qexp(0.99, 0.0000183)
data %>% group_by(STATE) %>% top_n(1)
data %>% group_by(STATE) %>% filter(row_number()==1)
test <- data %>% group_by(STATE) %>% filter(row_number()==1)
test
unqiue(test$COUNTYNAME)
test$COUNTYNAME
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data
qexp(0.99, rate=data$param)
length(c("Connecticut","New Jersey", "Massachusetts", "New York", "California", "Colorado", "Illinois", "Washington", "Maryland", "North Dakota", "Minnesota", "Texas", "Virginia", "Florida", "South Dakota", "Wyoming", "New Hampshire", "Alaska", "Pennsylvania", "Kansas", "Utah", "Georgia", "Nebraska", "Oregon", "Wisconsin","Rhode Island", "North Carolina", "Nevada", "Delaware", "Ohio", "Oklahoma", "Tennessee", "Iowa", "Arizona", "Michigan", "Missouri", "Vermont", "Montana", "South Carolina", "Louisiana", "Indiana", "Idaho", "Hawaii", "Maine", "Alabama", "Kentucky", "West Virginia", "New Mexico", "Arkansas", "Mississipi", "District of Columbia"))
states <- c("Connecticut","New Jersey", "Massachusetts", "New York", "California", "Colorado", "Illinois", "Washington", "Maryland", "North Dakota", "Minnesota", "Texas", "Virginia", "Florida", "South Dakota", "Wyoming", "New Hampshire", "Alaska", "Pennsylvania", "Kansas", "Utah", "Georgia", "Nebraska", "Oregon", "Wisconsin","Rhode Island", "North Carolina", "Nevada", "Delaware", "Ohio", "Oklahoma", "Tennessee", "Iowa", "Arizona", "Michigan", "Missouri", "Vermont", "Montana", "South Carolina", "Louisiana", "Indiana", "Idaho", "Hawaii", "Maine", "Alabama", "Kentucky", "West Virginia", "New Mexico", "Arkansas", "Mississipi", "District of Columbia")
match(states, data$COUNTYNAME)
states <- c("Connecticut","New Jersey", "Massachusetts", "New York", "California", "Colorado", "Illinois", "Washington", "Maryland", "North Dakota", "Minnesota", "Texas", "Virginia", "Florida", "South Dakota", "Wyoming", "New Hampshire", "Alaska", "Pennsylvania", "Kansas", "Utah", "Georgia", "Nebraska", "Oregon", "Wisconsin","Rhode Island", "North Carolina", "Nevada", "Delaware", "Ohio", "Oklahoma", "Tennessee", "Iowa", "Arizona", "Michigan", "Missouri", "Vermont", "Montana", "South Carolina", "Louisiana", "Indiana", "Idaho", "Hawaii", "Maine", "Alabama", "Kentucky", "West Virginia", "New Mexico", "Arkansas", "Mississippi", "District of Columbia")
match(states, data$COUNTYNAME)
order(match(states, data$COUNTYNAME))
truth <- c(700800,588575,582774,550174,514694,458576,456377,451395,445783,445415,443118,440758,425144, 416587,507406,405596,405286,400017,388593,375344,374467,371811,363310,358937,349905,346657,343066,341335,340770,334979,333139,332913,331572,33074, 328649, 326839,321969,321849,318463,318393,316756,314352,310566,303897, 297564,274818,258078,255429,255050,254362,598155)
length(trunc())
length(truth)
states <- c("Connecticut","New Jersey", "Massachusetts", "New York", "California", "Colorado", "Illinois", "Washington", "Maryland", "North Dakota", "Minnesota", "Texas", "Virginia", "Florida", "South Dakota", "Wyoming", "New Hampshire", "Alaska", "Pennsylvania", "Kansas", "Utah", "Georgia", "Nebraska", "Oregon", "Wisconsin","Rhode Island", "North Carolina", "Nevada", "Delaware", "Ohio", "Oklahoma", "Tennessee", "Iowa", "Arizona", "Michigan", "Missouri", "Vermont", "Montana", "South Carolina", "Louisiana", "Indiana", "Idaho", "Hawaii", "Maine", "Alabama", "Kentucky", "West Virginia", "New Mexico", "Arkansas", "Mississippi", "District of Columbia")
truth <- c(700800,588575,582774,550174,514694,458576,456377,451395,445783,445415,443118,440758,425144, 416587,507406,405596,405286,400017,388593,375344,374467,371811,363310,358937,349905,346657,343066,341335,340770,334979,333139,332913,331572,33074, 328649, 326839,321969,321849,318463,318393,316756,314352,310566,303897, 297564,274818,258078,255429,255050,254362,598155)
comp <- data.frame(states=states,epi.income=truth)
readRDS(comp, "/Users/aditharun/Desktop/other/income/comp_epi.rds")
comp
saveRDS(comp, "/Users/aditharun/Desktop/other/income/comp_epi.rds")
v
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds")
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp
comp$states <- as.character(comp$states)
comp
match(comp$states, data$COUNTYNAME)
data$test <- comp
data
data$test$states <- NULL
data$test$epi.income <- NULL
data
data$test <- NULL
data
data$test <- comp$states[match(comp$states, data$COUNTYNAME)]
data
data$test <- comp$states[match(data$COUNTYNAME, comp$states)]
data
data$test <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data
data$dev <- abs(data$epi.income - data$threshold) / data$epi.income
data
data %>% filter(dev > 1)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
com
comp
comp$epi.income[comp$states=="Arizona"] <- 331074
saveRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds")
saveRDS(comp, "/Users/aditharun/Desktop/other/income/comp_epi.rds")
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- abs(data$epi.income - data$threshold) / data$epi.income
data
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
data
cor(data$threshold, data$epi.income)
data %>% arrange(threshold, decreasing=TRUE)
data %>% arrange(threshold, decr=TRUE)
data %>% arrange(threshold)
data %>% arrange(threshold,decr=TRUE)
data %>% arrange(threshold,desc=TRUE)
data %>% arrange(desc(threshold))
data
data$X1 <- NULL
data
plot(data$threshold, data$epi.income)
abline(a=1,b=0)
abline(a=1,col="red")
abline(b=1)
cor(data$threshold, data$epi.income, method="spearman")
abline(b=1,col="red")
plot(data$threshold, data$epi.income)
abline(b=1,col="red")
abline(v=300000,col="red")
abline(b=1,a=0,col="red")
data
hist(data$dev, breaks=100)
hist(data$dev, breaks=10)
hist(data$dev, breaks=12)
plot(density(data$dev))
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
#normally distributed around 20% deviation looks like.
hist(data$dev, breaks=12)
#really high, like 0.86. spearman is even higher, like 0.91
cor <- cor(data$threshold, data$epi.income, method="pearson")
plot(data$threshold, data$epi.income)
data
data %>% arrange(desc(threshold))
data %>% arrange(desc(threshold)) %>% select(STATE)
data %>% arrange(desc(epi.income)) %>% select(STATE)
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
truth
guess
match(guess$STATE, truth$STATE)
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
ranks
plot(ranks$rank, ranks$corr.rank)
abline(a=0,b=1, col="blue")
ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed")
cor <- cor(data$threshold, data$epi.income, method="pearson")
ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + title(paste0("Correlation: ",cor))
cor <- cor(data$threshold, data$epi.income, method="pearson")
ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Correlation: ",cor))
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Correlation: ",cor))
hist(data$dev, breaks=12)
data
ggplot(ggplot(data, aes(dev)) + geom_histogram()
)
ggplot(data, aes(dev)) + geom_histogram()
ggplot(data, aes(dev)) + geom_histogram(bins=14)
ggplot(data, aes(dev)) + geom_histogram(bins=16)
ggplot(data, aes(dev)) + geom_histogram(bins=13)
ggplot(data, aes(dev)) + geom_histogram(bins=12)
ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")
ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimates")
ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
plot(data$threshold, data$epi.income)
ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
ggsave(c(p1,p2,p3),"/Users/aditharun/Desktop/test.pdf")
outputResults <- TRUE
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
if (outputResults){
pdf("/Users/aditharun/Desktop/test.pdf")
p1
p2
p3
dev.off()
}
p1
p2
#benchmark to 2015 data
outputResults <- TRUE
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
if (outputResults){
pdf("/Users/aditharun/Desktop/test.pdf")
p1
p2
p3
dev.off()
}
#benchmark to 2015 data
outputResults <- TRUE
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
if (outputResults){
png("/Users/aditharun/Desktop/test.png")
p1
p2
p3
dev.off()
}
outputResults
if (outputResults) {print("yes")}
pdf("/Users/aditharun/Desktop/test.pdf")
p1
p2
p3
dev.off()
if (outputResults){
pdf("/Users/aditharun/Desktop/test2.pdf")
p1
p2
p3
dev.off()
}
pdf("/Users/aditharun/Desktop/test2.pdf")
p1
p2
p3
dev.off()
cwd()
pwd
pwd()
getwd()
#benchmark to 2015 data
outputResults <- TRUE
directory <- getwd()
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
output <- paste0(directory,"/income_benchmark/test.pdf")
pdf(output)
p1
p2
p3
dev.off()
mkdir("rape")
dir.create("raoe")
outputResults <- TRUE
directory <- getwd()
outputdir <- paste0(directory, "/income_benchmark")
if (!dir.exists(outputdir)){
dir.create(outputdir)
}
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
outputfile <- paste0(outputdir,"/output.pdf")
pdf(output)
p1
p2
p3
dev.off()
#benchmark to 2015 data
outputResults <- TRUE
directory <- getwd()
outputdir <- paste0(directory, "/income_benchmark")
if (!dir.exists(outputdir)){
dir.create(outputdir)
}
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
outputfile <- paste0(outputdir,"/output.pdf")
pdf(outputfile)
p1
p2
p3
dev.off()
#benchmark to 2015 data
outputResults <- TRUE
directory <- getwd()
outputdir <- paste0(directory, "/income_benchmark")
if (!dir.exists(outputdir)){
dir.create(outputdir)
}
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
outputfile <- paste0(outputdir,"/output.pdf")
pdf(outputfile)
p1
p2
p3
dev.off()
saveRDS(data, paste0(outputdir,"/data.rds"))
#benchmark to 2015 data
outputResults <- TRUE
directory <- getwd()
outputdir <- paste0(directory, "/income_benchmark")
if (!dir.exists(outputdir)){
dir.create(outputdir)
}
library(tidyverse)
path <- "/Users/aditharun/Desktop/other/income/modeledData_2015.csv"
data <- read_csv(path)
data <- data %>% group_by(STATE) %>% filter(row_number()==1)
data$X1 <- NULL
data$threshold <- qexp(0.99, rate=data$param)
comp <- readRDS("/Users/aditharun/Desktop/other/income/comp_epi.rds") %>% as_tibble()
comp$states <- as.character(comp$states)
data$epi.income <- comp$epi.income[match(data$COUNTYNAME, comp$states)]
data$dev <- (abs(data$epi.income - data$threshold) / data$epi.income)*100
p1 <- ggplot(data, aes(x=threshold,y=epi.income)) + geom_point() + xlab("Estimated Income") + ylab("EPI Income") + ggtitle("Minimum Income to be considered top 1% in States")
p2 <- ggplot(data, aes(dev)) + geom_histogram(bins=12) + xlab("Percent Deviation of Estimate From Truth") + ylab("Frequency")  + ggtitle("Errors of Estimation Represented As Histogram")
guess <- data %>% arrange(desc(threshold)) %>% select(STATE)
truth <- data %>% arrange(desc(epi.income)) %>% select(STATE)
ranks <- data.frame(corr.rank=match(guess$STATE, truth$STATE), rank=c(1:51))
#spearman is even higher, like 0.91
cor <- round(cor(data$threshold, data$epi.income, method="pearson"),2)
p3 <- ggplot(ranks, aes(x=rank,y=corr.rank)) + geom_point() + xlab("Ordered Rank of EPI Income Results by State") + ylab("Corresponding Rank of State From Estimate") + geom_abline(slope=1,intercept=0,linetype="dashed") + ggtitle(paste0("Pearson Correlation Between Estimated Ranks and EPI Results: ",cor))
outputfile <- paste0(outputdir,"/output.pdf")
pdf(outputfile)
p1
p2
p3
dev.off()
saveRDS(data, paste0(outputdir,"/data.rds"))
library(shiny)
runApp("/Users/aditharun/Documents/income_distr/shiny_app/app.R")
